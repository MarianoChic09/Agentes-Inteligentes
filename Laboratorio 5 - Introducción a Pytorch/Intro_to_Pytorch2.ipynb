{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianoChic09/Agentes-Inteligentes/blob/main/Laboratorio%205%20-%20Introducci%C3%B3n%20a%20Pytorch/Intro_to_Pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLeDXUNk0zCH"
      },
      "source": [
        "# Pytorch\n",
        "\n",
        "https://pytorch.org/\n",
        "https://pytorch.org/tutorials/\n",
        "https://pytorch.org/docs/stable/index.html\n",
        "\n",
        "Pytorch es un framework de machine learning que nos permite rápidamente diseñar, entrenar y testear modelos de machine learning (en particular, redes neuronales). \n",
        "\n",
        "Vamos a utilizar este framework para implementar el obligatorio del curso, por eso, en la clase de hoy vamos a ver una breve introduccion al framework y las redes neuronales. Vamos a prestar detallada atencion a dos tipos de modelos: las redes FeedForward (neuronas que se conectan entre sí en una modalidad de \"cascada secuencial\").\n",
        "\n",
        "Este notebook debería servir como base para implementar todas las operaciones necesarias para resolver el obligatorio, así como tambien cualquier otra tarea básica de Deep Learning.\n",
        "\n",
        "### A Entregar:\n",
        "\n",
        "- Este mismo notebook con la solucion a todos los problemas planteados. Pueden trabajar en grupos de hasta 3 estudiantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoKE5aQswzaK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de tensores.\n",
        "Los tensores pueden crearse con listas o numpy arrays"
      ],
      "metadata": {
        "id": "G7MhfWbGxCZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "labMfKXmoh_N",
        "outputId": "76c98b14-0604-444a-b077-c25af1b915cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1., -1.],\n",
              "        [ 1., -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.tensor([[1., -1.], [1., -1.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3sjdOL4oiBf",
        "outputId": "4cc9a6a7-34dd-43ee-8569-9e366f87330c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCKc9UDoiDk",
        "outputId": "fdf8ff68-05f7-400a-dbe2-ce56e32a7a41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0],\n",
              "        [0, 0, 0, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.zeros([2, 4], dtype=torch.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piO2t-O1wzaM"
      },
      "source": [
        "### Manipulación de tensores.\n",
        "Los tensores pueden accederse mediante las directivas de slicing y e indexación de python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiAEqy3IwzaN",
        "outputId": "98c564a6-5bff-4c7f-d78c-9660422322e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6)\n",
            "tensor([[1, 8, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x[1][2])\n",
        "x[0][1] = 8\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHrP55WEwzaN"
      },
      "source": [
        "### Operaciones sobre tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZGs8ee8wzaN"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "y = torch.tensor(2)\n",
        "z = torch.randn(1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtLvPMTKwzaN",
        "outputId": "471d2794-04d4-425b-f58c-450c2fe4ec19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 4., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOTJdmP8wzaN",
        "outputId": "2687069b-ab17-4440-cf09-b3ccf62b9d3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "x * y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY1i2d2OwzaO",
        "outputId": "ddba5a52-127f-40f9-d31a-78d3aeafde44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5000, 1.0000, 1.5000])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x / y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnHa9gq-wzaO",
        "outputId": "8fcdbc20-0245-4d3a-9d6c-ad76b62e6feb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.7170])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "r = torch.mv(z, x) # producto vectorial\n",
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8pqRfApwzaO",
        "outputId": "cbf880f0-113b-4561-dd13-a0d93a1cfab7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6775,  1.7099, -2.5795,  2.8006],\n",
              "        [-0.1862, -2.3558,  0.1784,  2.4835]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "mat1 = torch.randn(2, 3)\n",
        "mat2 = torch.randn(3, 4)\n",
        "r = torch.mm(mat1, mat2) # producto matricial (matricial multiplication??)\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMjzwdkNwzaO"
      },
      "source": [
        "### Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhfN-4NPwzaP",
        "outputId": "1bdf776c-53bf-4417-c429-7ef007815309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "w = torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(w.size())                      \n",
        "print(torch.numel(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBhjVbRwzaP"
      },
      "source": [
        "### Resizing (reshaping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqY8n9newzaP",
        "outputId": "be85ca77-2024-47a0-8fea-7ca00ac2abc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of x: torch.Size([2, 3])\n",
            "Size of y: torch.Size([6])\n",
            "Size of z: torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 3)   \n",
        "print('Size of x:', x.size())\n",
        "y = x.view(6) # genera una vista de x con 6 elementos\n",
        "print('Size of y:', y.size())\n",
        "z = x.view(-1, 2) # genera una vista de x con 2 columnas (-1 es una wildcard es decir, lo que sea por 2)\n",
        "print('Size of z:', z.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GamkLndwzaP"
      },
      "source": [
        "Es una referencia, es decir cambia el elemento en memoria (si cambio y cambio x por ejemplo) -> Es una vista. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSK1Em_wwzaP"
      },
      "source": [
        "### Cálculo de gradientes\n",
        "Pytorch habilita al cálculo automático de gradientes (autograd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPqxe9VywzaQ",
        "outputId": "9a63b6b1-1ccb-4bd2-92de-905bfb6c25fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor(1., grad_fn=<MeanBackward0>)\n",
            "tensor([[ 0.5000, -0.5000],\n",
            "        [ 0.5000,  0.5000]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
        "print(x.grad)\n",
        "out = x.pow(2).mean()\n",
        "print(out)\n",
        "out.backward()\n",
        "\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hur5OGlgj-Ct"
      },
      "source": [
        "## Uso automático de GPU\n",
        "\n",
        "En Colab tenemos 12 Horas de GPU gratis para usar (cambiando el runtime type), esto nos permite entrenar modelos de DL mucho mas rápido. La celda de código abajo detecta si tenemos una GPU disponible o no y nos va a permitir escribir código genérico para cualquier dispositivo.\n",
        "\n",
        "***\n",
        "Recomendamos fuertemente utilizar CPU lo más posible mientras probamos código y usar la GPU solo para cuando sabemos que todo funciona y queremos obtener resultados. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh4qtotEwzaQ",
        "outputId": "583235d3-6633-4817-9400-e9347263c793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch CUDA Version is  11.8\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.is_available()\n",
        "print(\"Pytorch CUDA Version is \", torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpnJwoJPjiOE",
        "outputId": "c9db16de-80ac-448d-d35a-01b4f5b53b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbvuXJQAosTl",
        "outputId": "231be278-1078-4d94-d3c9-be3192eae3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU time:\n",
            "126 ms ± 15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            " \n",
            "GPU time:\n",
            "4.57 ms ± 151 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 900000).cpu()            # Initialize with random number (uniform distribution)\n",
        "y = torch.randn(900000,200).cpu()           # With normal distribution (SD=1, mean=0)\n",
        "z = torch.randperm(200).cpu()           # Size 200. Random permutation of integers from 0 to 200\n",
        "\n",
        "print('CPU time:')\n",
        "%timeit torch.mm(x,y)+z\n",
        "\n",
        "x = torch.rand(2, 900000).cuda()            \n",
        "y = torch.randn(900000,200).cuda()          \n",
        "z = torch.randperm(200).to(DEVICE)  # Manda al tensor al dispositivo que le pasamos (en este caso cuda:0)\n",
        "\n",
        "print(' ')\n",
        "print('GPU time:')\n",
        "%timeit torch.mm(x,y)+z "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "JpVIgjO52Uou",
        "outputId": "dfe33e46-63e0-4ca1-dbe7-1fe5d1ea5b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8699529.18it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/train-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 146148.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2713115.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18407738.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Tamaño del dataset 60000 imagenes.\n",
            "Clases posibles: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "Objeto imagen: <PIL.Image.Image image mode=L size=28x28 at 0x7FB14605F940> - Clase 9\n",
            "Detalles de la imagen (28, 28) pixeles\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "mnist_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True)\n",
        "\n",
        "print(f\"Tamaño del dataset {len(mnist_dataset)} imagenes.\")\n",
        "print(f\"Clases posibles: {mnist_dataset.classes}\")\n",
        "\n",
        "data_idx = 0  # Indice (0-59999) de la imagen que queremos ver\n",
        "image, label = mnist_dataset[0] \n",
        "\n",
        "print(f\"Objeto imagen: {image} - Clase {label}\")\n",
        "print(f\"Detalles de la imagen {image.size} pixeles\")\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl8qO_Sf2WZ1"
      },
      "source": [
        "## FeedForward networks\n",
        "\n",
        "Son la unidad más simple de red neuronal, con su origen en el perceptron de muchas capas. La idea es crear una secuencia lineal de neuronas (capa) que reciben nuestro input. \n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)\n",
        "\n",
        "De esta manera la primera capa de neuronas (input layer) recibe los datos y las capas subsiguientes reciben el resultados de capas anteriores. La última capa (output layer) es la encargada de generar una predicción a partir de nuestros inputs.\n",
        "\n",
        "***\n",
        "\n",
        "En este notebook vamos a usar un dataset muy simple y conocido de imágenes, Fashion-MNIST. Se trata de un dataset de ropa y calzado, la idea es usar redes neuronales para clasificar cada una de las imágenes el tipo de ropa que representa. \n",
        "\n",
        "Para trabajar con imagenes vamos a hacer uso de una librería complementaria a Pytorch: **torchvision** (https://pytorch.org/docs/stable/torchvision/index.html) que incluye varios datasets precargados, modelos preentrenados y algunas utilidades para trabajar con imágenes que nos van a resultar útiles.\n",
        "\n",
        "*** \n",
        "\n",
        "En la celda de abajo vamos a carga nuestro dataset y mostrar algunas imagenes de ejemplo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peWiTvQq2Uua"
      },
      "source": [
        "### Clasificador\n",
        "\n",
        "Ahora que tenemos una idea de como es nuestro dataset, vamos a crear un modelo FeedForward para predecir la clase de la imagen que usemos como input. \n",
        "\n",
        "Antes que nada, vamos a necesitar dividir el dataset total en conjuntos de **entrenamiento**, **validacion** y **test**. Vamos a usar un ratio de 80 y 20% respectivamente. El set de test se puede descargar por separado con torchvision. Además, vamos a necesitar una manera de cargar **batches** de datos a la vez, para entrenar nuestra red. Pytorch nos proporciona varias ayudas para esto.\n",
        "\n",
        "***\n",
        "\n",
        "Finalmente, queda aclarar el uso de **tranformaciones** sobre las imágenes. Por lo pronto, tenemos objetos de tipo PIL Image, necesitamos (al menos) convertirlos en Tensores, para que Pytorch los pueda manejar.\n",
        "\n",
        "Hay un numero inmenso de transformaciones posibles que podemos usar en nustras imagenes, en este caso basta con tranformarlas a tensores, pero dejamos este link para otros casos: https://pytorch.org/docs/stable/torchvision/transforms.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTFVIJZwwzaS"
      },
      "outputs": [],
      "source": [
        "#Esto nos permite cambiarle la forma a un tensor aplicandole una transformacion. \n",
        "\n",
        "class ReshapeTransform:\n",
        "    def __init__(self, new_size):\n",
        "        self.new_size = new_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return torch.reshape(img, self.new_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVBOfbYX2UzC"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "# primero vamos a entrenar un perceptron \n",
        "img_transforms = transforms.Compose([transforms.ToTensor(), ReshapeTransform((-1,))]) # Reshape con -1 va a aplanar la matriz. Hay que aplanar la imagen.\n",
        "\n",
        "# Descargamos los datasets\n",
        "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=img_transforms)\n",
        "\n",
        "# Separamos el train set en train y validation\n",
        "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
        "\n",
        "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=img_transforms)\n",
        "\n",
        "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
        "\n",
        "# Cuantas imagenes obtener en cada iteracion!\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Creamos los loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) # este carga los datos en la forma del batch size que defini, shuffle ordena aleatoriamente\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKgtfpeArQPd"
      },
      "source": [
        "### Modelo\n",
        "\n",
        "Vamos a considerar cada imagen como un tensor de una sola dimensión, de largo 28*28 = 784. Cada uno de esos valores representa el valor de un pixel de nuestra imagen original.\n",
        "\n",
        "Nuestra red va a recibir ese tensor como input (en realidad, un batch de tensores de largo 784) que va a ser trabajado por varias capas ocultas con diferente número de neuronas hasta llegar a una capa de salida con 10 outputs, 1 por cada clase posible.\n",
        "\n",
        "***\n",
        "\n",
        "Vamos utilizar capas conectadas totalmente, tambien conocidas como Fully Connected, Dense, o Linear en Pytorch (https://pytorch.org/docs/stable/nn.html). Para crearlas necesitamos especificar las dimensiones del tensor de entrada, y el de salida; luego internamente Pytorch genera la matriz de pesos por los cuales multiplicar la entrada para generar la salida. Luego de cada una de estas operaciones necesitamos usar una funcion de activacion no linear, en este caso, vamos a usar ReLU: https://pytorch.org/docs/stable/nn.html#relu. \n",
        "\n",
        "***\n",
        "\n",
        "Para implementar un modelo **cualquiera** alcanza con definir un metodo **init** donde especificamos la arquitectura del mismo, y un método **forward** donde especificamos cómo interactúan nuestras capas frente a un nuevo input.\n",
        "\n",
        "***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-SZey-Qqj1P",
        "outputId": "501c4d8c-ede6-4595-e017-8ef4c31eae43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForwardModel(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Definicion del modelo que vamos a usar. En Pytorch los modelos se definen como clases, que heredan de nn.Module\n",
        "import torch.nn as nn # Este nos va a dar las capas \n",
        "import torch.nn.functional as F # este nos da las funciones de activación. \n",
        "\n",
        "\n",
        "class FeedForwardModel(nn.Module):\n",
        "\n",
        "    def __init__(self, number_classes=10,input_features=784):\n",
        "        super(FeedForwardModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features=input_features, out_features=128) # 784*128 +128 (bias)\n",
        "        self.linear2 = nn.Linear(in_features=128, out_features=64) # 128*64 + 64 (bias)\n",
        "        self.output = nn.Linear(in_features=64, out_features=number_classes) #64*10 +10 (bias)\n",
        "  \n",
        "    def forward(self, new_input): # este es el que une todo, y ordena primero la ANN que tiene 784 entradas y 128 salidas, después 128 x 64 y despues 64 x number_classes\n",
        "        result = F.relu(self.linear1(new_input))\n",
        "        result = F.relu(self.linear2(result))\n",
        "        logits = self.output(result)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = FeedForwardModel(number_classes=10,input_features=28*28*1) # nos falta el softmax acá y la función de pérdida. \n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYv_kROa0o54"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Para entrenar un modelo necesitamos una funcion de costo o pérdida (normalmente referida como loss function: https://pytorch.org/docs/stable/nn.html#loss-functions). En este curso no nos vamos a meter en mucho detalle sobre las funciones de costo, para este ejercicio y el siguiente vamos a usar la CrossEntropyLoss, y cuando necesiten otra la vamos a especificar.\n",
        "\n",
        "El objetivo de esta funcion es darnos un valor de que tan malas fueron las predicciones del modelo respecto a los valores de verdad. Haciendo uso de backpropagation y del gradiente de esta funcion podemos optimizar los pesos de nuestra red tal que \"aprenda\" a hacer mejores predicciones. De nuevo, la lógica detras de toda esta optimización no nos compete en este curso y lo dejamos para la disciplina de Deep Learning.\n",
        "\n",
        "***\n",
        "Como mencionamos arriba, el costo de computa usando las predicciones del modelo y las etiquetas verdaderas de nuestros datos y, el trabajo de actualizar los pesos usando los gradientes lo realiza un optimizador de Pytorch: https://pytorch.org/docs/stable/optim.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYtCjd9cqj3x"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 0.003\n",
        "\n",
        "ff_model = FeedForwardModel(number_classes=10).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "ff_optimizer = optim.SGD(ff_model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBEV-LNsqj6o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, loss_func, optimizer, epochs):\n",
        "    for epoch in range(epochs):  # Iteramos sobre el dataset entero muchas veces\n",
        "\n",
        "        running_loss = 0.0  \n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Nuestros datos son imagenes y la clase de cada una.\n",
        "            images, labels = data\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Reseteamos los gradientes de los pesos del modelo.\n",
        "            optimizer.zero_grad()   \n",
        "\n",
        "            # Obtenemos las predicciones para las nuevas imagenes llamando a nuestro modelo.\n",
        "            predictions = model(images)    \n",
        "\n",
        "            # Calulamos el costo de nuestras predicciones respecto a la verdad\n",
        "            loss = loss_func(predictions, labels)\n",
        "\n",
        "            # Computamos los gradientes con backward y actualizamos los pesos con un optimizer.step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Estadísiticas\n",
        "            running_loss += loss.item()\n",
        "            if i % 500 == 499:    # Imprimimos luego de 1000 batches de datos\n",
        "                print(f\"Epoch: {epoch + 1}, Batch: {i + 1} - Loss: {running_loss / 500:.5f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "        # Luego de cada epoch de entrenamiento vemos la performance (accuracy) en el set de validacion\n",
        "        with torch.no_grad():\n",
        "            correct_predictions = 0.0\n",
        "\n",
        "            for i, data in enumerate(val_loader):\n",
        "                images, labels = data\n",
        "                images = images.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                predictions = model(images)\n",
        "                predictions = torch.argmax(predictions, dim=1)\n",
        "\n",
        "                correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
        "\n",
        "        print(f\"Validation accuracy {(100 * correct_predictions / len(val_loader.dataset)):.2f} %\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leKy45f4N0q8"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader):\n",
        "    # Finalmente Reportamos la performance en el test set:\n",
        "    with torch.no_grad():\n",
        "        correct_predictions = 0.0\n",
        "\n",
        "        for i, data in enumerate(test_loader):\n",
        "            images, labels = data\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            predictions = model(images)\n",
        "            predictions = torch.argmax(predictions, dim=1)\n",
        "\n",
        "            correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
        "\n",
        "    print(f\"Test set accuracy {(100 * correct_predictions / len(test_loader.dataset)):.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-V0wOf6wzaU",
        "outputId": "6caed0fe-1322-4109-996e-08508d5f99a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 500 - Loss: 1.24763\n",
            "Validation accuracy 74.86 %\n",
            "Epoch: 2, Batch: 500 - Loss: 0.57928\n",
            "Validation accuracy 81.64 %\n",
            "Epoch: 3, Batch: 500 - Loss: 0.50014\n",
            "Validation accuracy 82.99 %\n",
            "Epoch: 4, Batch: 500 - Loss: 0.46261\n",
            "Validation accuracy 83.81 %\n",
            "Epoch: 5, Batch: 500 - Loss: 0.43915\n",
            "Validation accuracy 84.53 %\n",
            "Epoch: 6, Batch: 500 - Loss: 0.42064\n",
            "Validation accuracy 85.42 %\n",
            "Epoch: 7, Batch: 500 - Loss: 0.40351\n",
            "Validation accuracy 85.16 %\n",
            "Epoch: 8, Batch: 500 - Loss: 0.38913\n",
            "Validation accuracy 85.99 %\n",
            "Epoch: 9, Batch: 500 - Loss: 0.37761\n",
            "Validation accuracy 86.42 %\n",
            "Epoch: 10, Batch: 500 - Loss: 0.37456\n",
            "Validation accuracy 86.83 %\n",
            "Epoch: 11, Batch: 500 - Loss: 0.35618\n",
            "Validation accuracy 86.57 %\n",
            "Epoch: 12, Batch: 500 - Loss: 0.34784\n",
            "Validation accuracy 87.42 %\n",
            "Epoch: 13, Batch: 500 - Loss: 0.33893\n",
            "Validation accuracy 87.08 %\n",
            "Epoch: 14, Batch: 500 - Loss: 0.32607\n",
            "Validation accuracy 87.12 %\n",
            "Epoch: 15, Batch: 500 - Loss: 0.31859\n",
            "Validation accuracy 87.48 %\n",
            "Test set accuracy 86.66 %\n"
          ]
        }
      ],
      "source": [
        "# Usando las funciones definidas arriba entrenar un modelo es trivial\n",
        "\n",
        "ff_model = train_model(ff_model, train_loader, val_loader, loss_func=criterion, optimizer=ff_optimizer, epochs=15)\n",
        "test_model(ff_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KBYY3PVwzaU"
      },
      "source": [
        "## Tarea 1\n",
        "\n",
        "Cree y entrene un modelo de red FeedForward que funcione mejor que el visto en clase. Puede usar lo que considere necesario (siempre dentro del mundo de redes feed forward - nada de convoluciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIyd6Fw_wzaU",
        "outputId": "0c4225db-bb9b-4b55-d29e-55e6faec2ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForwardModel2(\n",
              "  (linear1): Linear(in_features=784, out_features=392, bias=True)\n",
              "  (linear2): Linear(in_features=392, out_features=128, bias=True)\n",
              "  (linear3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Definicion del modelo que vamos a usar. En Pytorch los modelos se definen como clases, que heredan de nn.Module\n",
        "import torch.nn as nn # Este nos va a dar las capas \n",
        "import torch.nn.functional as F # este nos da las funciones de activación. \n",
        "\n",
        "\n",
        "class FeedForwardModel2(nn.Module):\n",
        "\n",
        "    def __init__(self, number_classes=10,input_features=784):\n",
        "        super(FeedForwardModel2, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features=input_features, out_features=392) # 784*128 +128 (bias)\n",
        "        self.linear2 = nn.Linear(in_features=392, out_features=128) # 784*128 +128 (bias)\n",
        "        self.linear3 = nn.Linear(in_features=128, out_features=64) # 128*64 + 64 (bias)\n",
        "        self.output = nn.Linear(in_features=64, out_features=number_classes) #64*10 +10 (bias)\n",
        "  \n",
        "    def forward(self, new_input): # este es el que une todo, y ordena primero la ANN que tiene 784 entradas y 128 salidas, después 128 x 64 y despues 64 x number_classes\n",
        "        result = F.relu(self.linear1(new_input))\n",
        "        result = F.relu(self.linear2(result))\n",
        "        result = F.relu(self.linear3(result))\n",
        "        logits = self.output(result)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = FeedForwardModel2(number_classes=10,input_features=28*28*1) # nos falta el softmax acá y la función de pérdida. \n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 0.003\n",
        "\n",
        "ff_model = FeedForwardModel2(number_classes=10).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "ff_optimizer = optim.SGD(ff_model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ],
      "metadata": {
        "id": "9yedxNFr-3-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynHlR3aLwzaU",
        "outputId": "448c6fd4-c257-42f7-d172-d4eb574956a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 500 - Loss: 1.57418\n",
            "Validation accuracy 70.88 %\n",
            "Epoch: 2, Batch: 500 - Loss: 0.63391\n",
            "Validation accuracy 80.62 %\n",
            "Epoch: 3, Batch: 500 - Loss: 0.51678\n",
            "Validation accuracy 81.43 %\n",
            "Epoch: 4, Batch: 500 - Loss: 0.46033\n",
            "Validation accuracy 84.05 %\n",
            "Epoch: 5, Batch: 500 - Loss: 0.43408\n",
            "Validation accuracy 82.49 %\n",
            "Epoch: 6, Batch: 500 - Loss: 0.40534\n",
            "Validation accuracy 84.87 %\n",
            "Epoch: 7, Batch: 500 - Loss: 0.38698\n",
            "Validation accuracy 85.71 %\n",
            "Epoch: 8, Batch: 500 - Loss: 0.36886\n",
            "Validation accuracy 86.02 %\n",
            "Epoch: 9, Batch: 500 - Loss: 0.35681\n",
            "Validation accuracy 86.58 %\n",
            "Epoch: 10, Batch: 500 - Loss: 0.34110\n",
            "Validation accuracy 86.66 %\n",
            "Epoch: 11, Batch: 500 - Loss: 0.32189\n",
            "Validation accuracy 87.88 %\n",
            "Epoch: 12, Batch: 500 - Loss: 0.31810\n",
            "Validation accuracy 88.12 %\n",
            "Epoch: 13, Batch: 500 - Loss: 0.31059\n",
            "Validation accuracy 87.39 %\n",
            "Epoch: 14, Batch: 500 - Loss: 0.29766\n",
            "Validation accuracy 87.47 %\n",
            "Epoch: 15, Batch: 500 - Loss: 0.28849\n",
            "Validation accuracy 88.07 %\n",
            "Test set accuracy 86.89 %\n"
          ]
        }
      ],
      "source": [
        "# Usando las funciones definidas arriba entrenar un modelo es trivial\n",
        "\n",
        "ff_model = train_model(ff_model, train_loader, val_loader, loss_func=criterion, optimizer=ff_optimizer, epochs=15)\n",
        "test_model(ff_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZBm-ka0wzaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef29nSSqwzaU"
      },
      "source": [
        "# Redes Convolucionales\n",
        "\n",
        "\n",
        "![Image](https://www.unite.ai/wp-content/uploads/2019/12/Typical_cnn-1.png)\n",
        "\n",
        "\n",
        "Las redes convolucionales (CNNs) se basan en el uso de una tecnica muy usada en el campo de computer vision tradicional, las **convoluciones** (https://en.wikipedia.org/wiki/Kernel_(image_processing), la idea es crear un filtro pequeño que pasamos por encima de toda la imagen y nos permite detectar distintos elementos (como son líneas verticales, horizontales, diagonales, circulos, etc). EL gran problema de las convoluciones es que para crear dichos filtros debemos poder especificar distintos valores (pesos) para cada región en el mismo. \n",
        "\n",
        "Cada filtro (tambien conocido como kernel) nos permite identificar algo en particular en la imágen, y aplicar un filtro al resultado de otro (u otros) nos permite obtener informacion de más alto nivel (como por ejemplo detectar ojos, ruedas, puertas, etc).\n",
        "\n",
        "![Image](https://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "***\n",
        "\n",
        "Las redes convolucionales nos dan una manera de no sólo aprender los vaores óptimos para dichos filtros (mediante backprop) sino tambien la posibilidad de hacerlo a escala usndo un número arbitrario de los mismos. Una gran ventaja que nos trae el uso de filtros, es el hecho de que requieren de un número muy chico de pesos a entrenar, lo que reduce el tamaño de nuestra red y nos permite entrenar mas rápido (o redes mas grandes y profundas con el mismo hardware).\n",
        "\n",
        "Una cosa a notar en las redes convolucionales es el hecho de que las imágenes se van reduciendo en su tamaño a medida que fluyen por la red, esto se debe a la opeación de `maxpooling` que toma regiones (por lo general de 2x2) en nuestra imagen y se queda con el valor más alto en la zona, reduciendo asi el tamaño de la imagen. El resultado de aplicar un filtro de convolución a una imagen se llama `feature_map` y se puede pensar como otra imagen que describe la características de la original. \n",
        "\n",
        "***\n",
        "\n",
        "Al final de nuestra red, necesitamos formar una predicción de la clase de nuestra imagen, por lo que tenemos que \"achatar\" estos feature maps y pasarlos por una (o varias) capas lineales que generen una predicción. Esto se puede ver como representar toda la informacion que conocemos de la imagen, como por ejemplo si tiene nariz, orejas, pelo, en un sólo vector; y decidir que ese vector representa a un perro.\n",
        "\n",
        "***\n",
        "\n",
        "Para empezar, volvemos a definir nuestros conjuntos de datos. Esta vez, sin hacer uso de ninguna transformacion sobre la imagen (mas que transformarla en un tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ZyMqnQwzaV"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Descargamos los datasets\n",
        "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Separamos el train set en train y validation\n",
        "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
        "\n",
        "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
        "\n",
        "# Cuantas imagenes obtener en cada iteracion!\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Creamos los loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ziZzhHuwzaV"
      },
      "source": [
        "### Modelo Convolucional\n",
        "\n",
        "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arcquitectura y comportamiento de los componentes del modelo. \n",
        "\n",
        "En particular vamos a usar:\n",
        "\n",
        "- capas convolucionales de 2D (https://pytorch.org/docs/stable/nn.html#conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes). \n",
        "\n",
        "- Capas de maxpooling (https://pytorch.org/docs/stable/nn.html#maxpool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
        "\n",
        "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZyHLCAQwzaV"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalModel(nn.Module):\n",
        "    def __init__(self, number_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1) # kernel_size el tamaño de la matriz que arrastro sobre la imagen. padding agrega valores en las esquinas para que entre el kernel y no nos quite filas y columnas, es decir no cambie el tamaño de la imagen. \n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
        "        \n",
        "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
        "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
        "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=7*7*8, out_features=128) # Imagenes de 7*7 con 8 canales. El 128 es porque queda bien, se podria cambiar. \n",
        "        \n",
        "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, new_input):\n",
        "        result = self.conv1(new_input)\n",
        "        result = F.relu(self.pooling_layer(result))\n",
        "        \n",
        "        result = self.conv2(result)\n",
        "        result = F.relu(self.pooling_layer(result))     \n",
        "        \n",
        "        # \"Achatamos\" los feature maps\n",
        "        result = result.reshape((-1, self.fc1.in_features))\n",
        "        result = F.relu(self.fc1(result))\n",
        "        \n",
        "        return self.output(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlTVOzGwwzaV",
        "outputId": "b7e36f8c-d3c8-4eaa-f30d-ec456e1416da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 500 - Loss: 0.70993\n",
            "Validation accuracy 86.57 %\n",
            "Epoch: 2, Batch: 500 - Loss: 0.36136\n",
            "Validation accuracy 87.62 %\n",
            "Epoch: 3, Batch: 500 - Loss: 0.31056\n",
            "Validation accuracy 88.40 %\n",
            "Epoch: 4, Batch: 500 - Loss: 0.28943\n",
            "Validation accuracy 89.32 %\n",
            "Epoch: 5, Batch: 500 - Loss: 0.26526\n",
            "Validation accuracy 89.58 %\n",
            "Epoch: 6, Batch: 500 - Loss: 0.25079\n",
            "Validation accuracy 90.01 %\n",
            "Epoch: 7, Batch: 500 - Loss: 0.23928\n",
            "Validation accuracy 89.78 %\n",
            "Epoch: 8, Batch: 500 - Loss: 0.22765\n",
            "Validation accuracy 90.29 %\n",
            "Epoch: 9, Batch: 500 - Loss: 0.22020\n",
            "Validation accuracy 90.56 %\n",
            "Epoch: 10, Batch: 500 - Loss: 0.20514\n",
            "Validation accuracy 90.99 %\n",
            "Test set accuracy 90.58 %\n"
          ]
        }
      ],
      "source": [
        "conv_model = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
        "\n",
        "LEARNING_RATE = 0.03\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "conv_model = train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=10)\n",
        "test_model(conv_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMfSbaWRFfd_",
        "outputId": "d905869e-b52b-485c-f8cc-c5f306d7c367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.9/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "YSrz09raFi7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(conv_model,(1,28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVCHwZqhFmqg",
        "outputId": "f33951dd-57aa-41ff-d4a4-84556b257ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             160\n",
            "         MaxPool2d-2           [-1, 16, 14, 14]               0\n",
            "            Conv2d-3            [-1, 8, 14, 14]           1,160\n",
            "         MaxPool2d-4              [-1, 8, 7, 7]               0\n",
            "            Linear-5                  [-1, 128]          50,304\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 52,914\n",
            "Trainable params: 52,914\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.14\n",
            "Params size (MB): 0.20\n",
            "Estimated Total Size (MB): 0.34\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(ff_model,(1*28*28,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bm01idJFrko",
        "outputId": "e3a641ed-a881-4986-ee8b-73736f311174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 392]         307,720\n",
            "            Linear-2                  [-1, 128]          50,304\n",
            "            Linear-3                   [-1, 64]           8,256\n",
            "            Linear-4                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 366,930\n",
            "Trainable params: 366,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 1.40\n",
            "Estimated Total Size (MB): 1.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5nIegzawzaV"
      },
      "source": [
        "## Tarea 2\n",
        "\n",
        "Cree y entrene un modelo de red Convolucional que funcione mejor que el visto en clase. Puede usar lo que considere necesario."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)***Modificar esto de abajo: ***"
      ],
      "metadata": {
        "id": "7gqwphlhElet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qaMBLtJwzaW"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalModel(nn.Module):\n",
        "    def __init__(self, number_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1) # kernel_size el tamaño de la matriz que arrastro sobre la imagen. padding agrega valores en las esquinas para que entre el kernel y no nos quite filas y columnas, es decir no cambie el tamaño de la imagen. \n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
        "        \n",
        "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
        "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
        "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=7*7*8, out_features=128) # Imagenes de 7*7 con 8 canales. El 128 es porque queda bien, se podria cambiar. \n",
        "        \n",
        "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, new_input):\n",
        "        result = self.conv1(new_input)\n",
        "        result = F.relu(self.pooling_layer(result))\n",
        "        \n",
        "        result = self.conv2(result)\n",
        "        result = F.relu(self.pooling_layer(result))     \n",
        "        \n",
        "        # \"Achatamos\" los feature maps\n",
        "        result = result.reshape((-1, self.fc1.in_features))\n",
        "        result = F.relu(self.fc1(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
        "\n",
        "LEARNING_RATE = 0.03\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "conv_model = train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=10)\n",
        "test_model(conv_model, test_loader)"
      ],
      "metadata": {
        "id": "43rtcv3kEcHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5chIYzNwzaW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hda421LbwzaW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}