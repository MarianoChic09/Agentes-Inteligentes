{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLeDXUNk0zCH"
   },
   "source": [
    "# Pytorch\n",
    "\n",
    "https://pytorch.org/\n",
    "https://pytorch.org/tutorials/\n",
    "https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "Pytorch es un framework de machine learning que nos permite rápidamente diseñar, entrenar y testear modelos de machine learning (en particular, redes neuronales). \n",
    "\n",
    "Vamos a utilizar este framework para implementar el obligatorio del curso, por eso, en la clase de hoy vamos a ver una breve introduccion al framework y las redes neuronales. Vamos a prestar detallada atencion a dos tipos de modelos: las redes FeedForward (neuronas que se conectan entre sí en una modalidad de \"cascada secuencial\").\n",
    "\n",
    "Este notebook debería servir como base para implementar todas las operaciones necesarias para resolver el obligatorio, así como tambien cualquier otra tarea básica de Deep Learning.\n",
    "\n",
    "### A Entregar:\n",
    "\n",
    "- Este mismo notebook con la solucion a todos los problemas planteados. Pueden trabajar en grupos de hasta 3 estudiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocrqKXBzohxf"
   },
   "source": [
    "### Creación de tensores.\n",
    "Los tensores pueden crearse con listas o numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "labMfKXmoh_N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.],\n",
       "        [ 1., -1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1., -1.], [1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3sjdOL4oiBf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBCKc9UDoiDk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([2, 4], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de tensores.\n",
    "Los tensores pueden accederse mediante las directivas de slicing y e indexación de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([[1, 8, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x[1][2])\n",
    "x[0][1] = 8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones sobre tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor(2)\n",
    "z = torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.0000, 1.5000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.8974])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.mv(z, x) # producto vectorial\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8801,  0.6316,  2.5625, -0.8492],\n",
       "        [-1.8961,  0.4813,  4.5307, -0.3655]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 4)\n",
    "r = torch.mm(mat1, mat2) # producto matricial (matricial multiplication??)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(w.size())                      \n",
    "print(torch.numel(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing (reshaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x: torch.Size([2, 3])\n",
      "Size of y: torch.Size([6])\n",
      "Size of z: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)   \n",
    "print('Size of x:', x.size())\n",
    "y = x.view(6) # genera una vista de x con 6 elementos\n",
    "print('Size of y:', y.size())\n",
    "z = x.view(-1, 2) # genera una vista de x con 2 columnas (-1 es una wildcard es decir, lo que sea por 2)\n",
    "print('Size of z:', z.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una referencia, es decir cambia el elemento en memoria (si cambio y cambio x por ejemplo) -> Es una vista. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de gradientes\n",
    "Pytorch habilita al cálculo automático de gradientes (autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(1., grad_fn=<MeanBackward0>)\n",
      "tensor([[ 0.5000, -0.5000],\n",
      "        [ 0.5000,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "print(x.grad)\n",
    "out = x.pow(2).mean()\n",
    "print(out)\n",
    "out.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hur5OGlgj-Ct"
   },
   "source": [
    "## Uso automático de GPU\n",
    "\n",
    "En Colab tenemos 12 Horas de GPU gratis para usar (cambiando el runtime type), esto nos permite entrenar modelos de DL mucho mas rápido. La celda de código abajo detecta si tenemos una GPU disponible o no y nos va a permitir escribir código genérico para cualquier dispositivo.\n",
    "\n",
    "***\n",
    "Recomendamos fuertemente utilizar CPU lo más posible mientras probamos código y usar la GPU solo para cuando sabemos que todo funciona y queremos obtener resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA Version is  None\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "print(\"Pytorch CUDA Version is \", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpnJwoJPjiOE",
    "outputId": "e21f35b0-4b0b-43ef-80e4-c842418cb66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbvuXJQAosTl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time:\n",
      "169 ms ± 8.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCPU time:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mtimeit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtorch.mm(x,y)+z\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m2\u001b[39;49m, \u001b[39m900000\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()            \n\u001b[0;32m      9\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m900000\u001b[39m,\u001b[39m200\u001b[39m)\u001b[39m.\u001b[39mcuda()          \n\u001b[0;32m     10\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(\u001b[39m200\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)  \u001b[39m# Manda al tensor al dispositivo que le pasamos (en este caso cuda:0)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 900000).cpu()            # Initialize with random number (uniform distribution)\n",
    "y = torch.randn(900000,200).cpu()           # With normal distribution (SD=1, mean=0)\n",
    "z = torch.randperm(200).cpu()           # Size 200. Random permutation of integers from 0 to 200\n",
    "\n",
    "print('CPU time:')\n",
    "%timeit torch.mm(x,y)+z\n",
    "\n",
    "x = torch.rand(2, 900000).cuda()            \n",
    "y = torch.randn(900000,200).cuda()          \n",
    "z = torch.randperm(200).to(DEVICE)  # Manda al tensor al dispositivo que le pasamos (en este caso cuda:0)\n",
    "\n",
    "print(' ')\n",
    "print('GPU time:')\n",
    "%timeit torch.mm(x,y)+z  +x x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nl8qO_Sf2WZ1"
   },
   "source": [
    "## FeedForward networks\n",
    "\n",
    "Son la unidad más simple de red neuronal, con su origen en el perceptron de muchas capas. La idea es crear una secuencia lineal de neuronas (capa) que reciben nuestro input. \n",
    "\n",
    "![Image](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)\n",
    "\n",
    "De esta manera la primera capa de neuronas (input layer) recibe los datos y las capas subsiguientes reciben el resultados de capas anteriores. La última capa (output layer) es la encargada de generar una predicción a partir de nuestros inputs.\n",
    "\n",
    "***\n",
    "\n",
    "En este notebook vamos a usar un dataset muy simple y conocido de imágenes, Fashion-MNIST. Se trata de un dataset de ropa y calzado, la idea es usar redes neuronales para clasificar cada una de las imágenes el tipo de ropa que representa. \n",
    "\n",
    "Para trabajar con imagenes vamos a hacer uso de una librería complementaria a Pytorch: **torchvision** (https://pytorch.org/docs/stable/torchvision/index.html) que incluye varios datasets precargados, modelos preentrenados y algunas utilidades para trabajar con imágenes que nos van a resultar útiles.\n",
    "\n",
    "*** \n",
    "\n",
    "En la celda de abajo vamos a carga nuestro dataset y mostrar algunas imagenes de ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "JpVIgjO52Uou",
    "outputId": "1efde127-8277-401d-af9d-d7bfb3129a96"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_get_cpp_backtrace' from 'torch._C' (c:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\_C.cp310-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdsets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mnist_dataset \u001b[39m=\u001b[39m dsets\u001b[39m.\u001b[39mFashionMNIST(\u001b[39m\"\u001b[39m\u001b[39mruta_donde_guardar_datos\u001b[39m\u001b[39m\"\u001b[39m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTamaño del dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(mnist_dataset)\u001b[39m}\u001b[39;00m\u001b[39m imagenes.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39malexnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdensenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mefficientnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstochastic_depth\u001b[39;00m \u001b[39mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_presets\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torchvision\\ops\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_register_onnx_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m _register_custom_op\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mboxes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     batched_nms,\n\u001b[0;32m      4\u001b[0m     box_area,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     remove_small_boxes,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mciou_loss\u001b[39;00m \u001b[39mimport\u001b[39;00m complete_box_iou_loss\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torchvision\\ops\\_register_onnx_ops.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m symbolic_opset11 \u001b[39mas\u001b[39;00m opset11\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymbolic_helper\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_args\n\u001b[0;32m      8\u001b[0m _ONNX_OPSET_VERSION_11 \u001b[39m=\u001b[39m \u001b[39m11\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\onnx\\symbolic_opset11.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m _C\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m _onnx \u001b[39mas\u001b[39;00m _C_onnx\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     _type_utils,\n\u001b[0;32m     14\u001b[0m     errors,\n\u001b[0;32m     15\u001b[0m     symbolic_helper,\n\u001b[0;32m     16\u001b[0m     symbolic_opset10 \u001b[39mas\u001b[39;00m opset10,\n\u001b[0;32m     17\u001b[0m     symbolic_opset9 \u001b[39mas\u001b[39;00m opset9,\n\u001b[0;32m     18\u001b[0m     utils,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_globals\u001b[39;00m \u001b[39mimport\u001b[39;00m GLOBALS\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m _beartype, jit_utils, registration\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\onnx\\_type_utils.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m _onnx \u001b[39mas\u001b[39;00m _C_onnx\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m errors\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m _beartype\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m typing\u001b[39m.\u001b[39mTYPE_CHECKING:\n\u001b[0;32m     15\u001b[0m     \u001b[39m# Hack to help mypy to recognize torch._C.Value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\onnx\\errors.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m _C\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m _constants\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m diagnostics\n\u001b[0;32m     11\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOnnxExporterError\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOnnxExporterWarning\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSymbolicValueError\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mOnnxExporterWarning\u001b[39;00m(\u001b[39mUserWarning\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_diagnostic\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     create_export_diagnostic_context,\n\u001b[0;32m      3\u001b[0m     diagnose,\n\u001b[0;32m      4\u001b[0m     engine,\n\u001b[0;32m      5\u001b[0m     export_context,\n\u001b[0;32m      6\u001b[0m     ExportDiagnostic,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_rules\u001b[39;00m \u001b[39mimport\u001b[39;00m rules\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minfra\u001b[39;00m \u001b[39mimport\u001b[39;00m levels\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\_diagnostic.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiagnostics\u001b[39;00m \u001b[39mimport\u001b[39;00m infra\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m cpp_backtrace\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cpp_call_stack\u001b[39m(frames_to_skip: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, frames_to_log: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m infra\u001b[39m.\u001b[39mStack:\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\"\"Returns the current C++ call stack.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39m    This function utilizes `torch.utils.cpp_backtrace` to get the current C++ call stack.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\utils\\cpp_backtrace.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_cpp_backtrace\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cpp_backtrace\u001b[39m(frames_to_skip\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, maximum_number_of_frames\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    Returns a string containing the C++ stack trace of the current thread.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m        frames_to_skip (int): the number of frames to skip from the top of the stack\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m        maximum_number_of_frames (int): the maximum number of frames to return\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_get_cpp_backtrace' from 'torch._C' (c:\\Users\\maria\\.conda\\envs\\DS\\lib\\site-packages\\torch\\_C.cp310-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "mnist_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True)\n",
    "\n",
    "print(f\"Tamaño del dataset {len(mnist_dataset)} imagenes.\")\n",
    "print(f\"Clases posibles: {mnist_dataset.classes}\")\n",
    "\n",
    "data_idx = 0  # Indice (0-59999) de la imagen que queremos ver\n",
    "image, label = mnist_dataset[0] \n",
    "\n",
    "print(f\"Objeto imagen: {image} - Clase {label}\")\n",
    "print(f\"Detalles de la imagen {image.size} pixeles\")\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peWiTvQq2Uua"
   },
   "source": [
    "### Clasificador\n",
    "\n",
    "Ahora que tenemos una idea de como es nuestro dataset, vamos a crear un modelo FeedForward para predecir la clase de la imagen que usemos como input. \n",
    "\n",
    "Antes que nada, vamos a necesitar dividir el dataset total en conjuntos de **entrenamiento**, **validacion** y **test**. Vamos a usar un ratio de 80 y 20% respectivamente. El set de test se puede descargar por separado con torchvision. Además, vamos a necesitar una manera de cargar **batches** de datos a la vez, para entrenar nuestra red. Pytorch nos proporciona varias ayudas para esto.\n",
    "\n",
    "***\n",
    "\n",
    "Finalmente, queda aclarar el uso de **tranformaciones** sobre las imágenes. Por lo pronto, tenemos objetos de tipo PIL Image, necesitamos (al menos) convertirlos en Tensores, para que Pytorch los pueda manejar.\n",
    "\n",
    "Hay un numero inmenso de transformaciones posibles que podemos usar en nustras imagenes, en este caso basta con tranformarlas a tensores, pero dejamos este link para otros casos: https://pytorch.org/docs/stable/torchvision/transforms.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto nos permite cambiarle la forma a un tensor aplicandole una transformacion. \n",
    "\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return torch.reshape(img, self.new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVBOfbYX2UzC"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_transforms = transforms.Compose([transforms.ToTensor(), ReshapeTransform((-1,))])\n",
    "\n",
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=img_transforms)\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=img_transforms)\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKgtfpeArQPd"
   },
   "source": [
    "### Modelo\n",
    "\n",
    "Vamos a considerar cada imagen como un tensor de una sola dimensión, de largo 28*28 = 784. Cada uno de esos valores representa el valor de un pixel de nuestra imagen original.\n",
    "\n",
    "Nuestra red va a recibir ese tensor como input (en realidad, un batch de tensores de largo 784) que va a ser trabajado por varias capas ocultas con diferente número de neuronas hasta llegar a una capa de salida con 10 outputs, 1 por cada clase posible.\n",
    "\n",
    "***\n",
    "\n",
    "Vamos utilizar capas conectadas totalmente, tambien conocidas como Fully Connected, Dense, o Linear en Pytorch (https://pytorch.org/docs/stable/nn.html). Para crearlas necesitamos especificar las dimensiones del tensor de entrada, y el de salida; luego internamente Pytorch genera la matriz de pesos por los cuales multiplicar la entrada para generar la salida. Luego de cada una de estas operaciones necesitamos usar una funcion de activacion no linear, en este caso, vamos a usar ReLU: https://pytorch.org/docs/stable/nn.html#relu. \n",
    "\n",
    "***\n",
    "\n",
    "Para implementar un modelo **cualquiera** alcanza con definir un metodo **init** donde especificamos la arquitectura del mismo, y un método **forward** donde especificamos cómo interactúan nuestras capas frente a un nuevo input.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "h-SZey-Qqj1P",
    "outputId": "c3466114-d313-4b5c-c589-e6027ac4fe6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardModel(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definicion del modelo que vamos a usar. En Pytorch los modelos se definen como clases, que heredan de nn.Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, number_classes=10):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=784, out_features=128)\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.output = nn.Linear(in_features=64, out_features=number_classes)\n",
    "  \n",
    "    def forward(self, new_input):\n",
    "        result = F.relu(self.linear1(new_input))\n",
    "        result = F.relu(self.linear2(result))\n",
    "        logits = self.output(result)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = FeedForwardModel(number_classes=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYv_kROa0o54"
   },
   "source": [
    "### Entrenando el modelo\n",
    "\n",
    "Para entrenar un modelo necesitamos una funcion de costo o pérdida (normalmente referida como loss function: https://pytorch.org/docs/stable/nn.html#loss-functions). En este curso no nos vamos a meter en mucho detalle sobre las funciones de costo, para este ejercicio y el siguiente vamos a usar la CrossEntropyLoss, y cuando necesiten otra la vamos a especificar.\n",
    "\n",
    "El objetivo de esta funcion es darnos un valor de que tan malas fueron las predicciones del modelo respecto a los valores de verdad. Haciendo uso de backpropagation y del gradiente de esta funcion podemos optimizar los pesos de nuestra red tal que \"aprenda\" a hacer mejores predicciones. De nuevo, la lógica detras de toda esta optimización no nos compete en este curso y lo dejamos para la disciplina de Deep Learning.\n",
    "\n",
    "***\n",
    "Como mencionamos arriba, el costo de computa usando las predicciones del modelo y las etiquetas verdaderas de nuestros datos y, el trabajo de actualizar los pesos usando los gradientes lo realiza un optimizador de Pytorch: https://pytorch.org/docs/stable/optim.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYtCjd9cqj3x"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "ff_model = FeedForwardModel(number_classes=10).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "ff_optimizer = optim.SGD(ff_model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "zBEV-LNsqj6o",
    "outputId": "7965009c-f8c9-420c-c3b0-0a290420156b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_func, optimizer, epochs):\n",
    "    for epoch in range(epochs):  # Iteramos sobre el dataset entero muchas veces\n",
    "\n",
    "        running_loss = 0.0  \n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Nuestros datos son imagenes y la clase de cada una.\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Reseteamos los gradientes de los pesos del modelo.\n",
    "            optimizer.zero_grad()   \n",
    "\n",
    "            # Obtenemos las predicciones para las nuevas imagenes llamando a nuestro modelo.\n",
    "            predictions = model(images)    \n",
    "\n",
    "            # Calulamos el costo de nuestras predicciones respecto a la verdad\n",
    "            loss = loss_func(predictions, labels)\n",
    "\n",
    "            # Computamos los gradientes con backward y actualizamos los pesos con un optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Estadísiticas\n",
    "            running_loss += loss.item()\n",
    "            if i % 500 == 499:    # Imprimimos luego de 1000 batches de datos\n",
    "                print(f\"Epoch: {epoch + 1}, Batch: {i + 1} - Loss: {running_loss / 500:.5f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        # Luego de cada epoch de entrenamiento vemos la performance (accuracy) en el set de validacion\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0.0\n",
    "\n",
    "            for i, data in enumerate(val_loader):\n",
    "                images, labels = data\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                predictions = model(images)\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "                correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "        print(f\"Validation accuracy {(100 * correct_predictions / len(val_loader.dataset)):.2f} %\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "leKy45f4N0q8",
    "outputId": "24580975-aa42-4f23-9f2c-3f76e9a25075"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    # Finalmente Reportamos la performance en el test set:\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            predictions = model(images)\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "            correct_predictions += (predictions == labels).detach().cpu().float().sum().item()\n",
    "\n",
    "    print(f\"Test set accuracy {(100 * correct_predictions / len(test_loader.dataset)):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 1.32283\n",
      "Validation accuracy 76.46 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.58513\n",
      "Validation accuracy 81.41 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.49480\n",
      "Validation accuracy 83.43 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.45560\n",
      "Validation accuracy 84.33 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.42961\n",
      "Validation accuracy 84.33 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.41248\n",
      "Validation accuracy 84.59 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.39521\n",
      "Validation accuracy 85.32 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.38107\n",
      "Validation accuracy 86.29 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.37169\n",
      "Validation accuracy 86.68 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.35991\n",
      "Validation accuracy 85.56 %\n",
      "Epoch: 11, Batch: 500 - Loss: 0.35213\n",
      "Validation accuracy 86.53 %\n",
      "Epoch: 12, Batch: 500 - Loss: 0.34323\n",
      "Validation accuracy 86.99 %\n",
      "Epoch: 13, Batch: 500 - Loss: 0.33577\n",
      "Validation accuracy 87.02 %\n",
      "Epoch: 14, Batch: 500 - Loss: 0.32808\n",
      "Validation accuracy 87.64 %\n",
      "Epoch: 15, Batch: 500 - Loss: 0.31548\n",
      "Validation accuracy 87.58 %\n",
      "Test set accuracy 86.74 %\n"
     ]
    }
   ],
   "source": [
    "# Usando las funciones definidas arriba entrenar un modelo es trivial\n",
    "\n",
    "ff_model = train_model(ff_model, train_loader, val_loader, loss_func=criterion, optimizer=ff_optimizer, epochs=15)\n",
    "test_model(ff_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 1\n",
    "\n",
    "Cree y entrene un modelo de red FeedForward que funcione mejor que el visto en clase. Puede usar lo que considere necesario (siempre dentro del mundo de redes feed forward - nada de convoluciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Convolucionales\n",
    "\n",
    "\n",
    "![Image](https://www.unite.ai/wp-content/uploads/2019/12/Typical_cnn-1.png)\n",
    "\n",
    "\n",
    "Las redes convolucionales (CNNs) se basan en el uso de una tecnica muy usada en el campo de computer vision tradicional, las **convoluciones** (https://en.wikipedia.org/wiki/Kernel_(image_processing), la idea es crear un filtro pequeño que pasamos por encima de toda la imagen y nos permite detectar distintos elementos (como son líneas verticales, horizontales, diagonales, circulos, etc). EL gran problema de las convoluciones es que para crear dichos filtros debemos poder especificar distintos valores (pesos) para cada región en el mismo. \n",
    "\n",
    "Cada filtro (tambien conocido como kernel) nos permite identificar algo en particular en la imágen, y aplicar un filtro al resultado de otro (u otros) nos permite obtener informacion de más alto nivel (como por ejemplo detectar ojos, ruedas, puertas, etc).\n",
    "\n",
    "![Image](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "***\n",
    "\n",
    "Las redes convolucionales nos dan una manera de no sólo aprender los vaores óptimos para dichos filtros (mediante backprop) sino tambien la posibilidad de hacerlo a escala usndo un número arbitrario de los mismos. Una gran ventaja que nos trae el uso de filtros, es el hecho de que requieren de un número muy chico de pesos a entrenar, lo que reduce el tamaño de nuestra red y nos permite entrenar mas rápido (o redes mas grandes y profundas con el mismo hardware).\n",
    "\n",
    "Una cosa a notar en las redes convolucionales es el hecho de que las imágenes se van reduciendo en su tamaño a medida que fluyen por la red, esto se debe a la opeación de `maxpooling` que toma regiones (por lo general de 2x2) en nuestra imagen y se queda con el valor más alto en la zona, reduciendo asi el tamaño de la imagen. El resultado de aplicar un filtro de convolución a una imagen se llama `feature_map` y se puede pensar como otra imagen que describe la características de la original. \n",
    "\n",
    "***\n",
    "\n",
    "Al final de nuestra red, necesitamos formar una predicción de la clase de nuestra imagen, por lo que tenemos que \"achatar\" estos feature maps y pasarlos por una (o varias) capas lineales que generen una predicción. Esto se puede ver como representar toda la informacion que conocemos de la imagen, como por ejemplo si tiene nariz, orejas, pelo, en un sólo vector; y decidir que ese vector representa a un perro.\n",
    "\n",
    "***\n",
    "\n",
    "Para empezar, volvemos a definir nuestros conjuntos de datos. Esta vez, sin hacer uso de ninguna transformacion sobre la imagen (mas que transformarla en un tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Descargamos los datasets\n",
    "mnist_train_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Separamos el train set en train y validation\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_train_dataset, [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))])\n",
    "\n",
    "mnist_test_dataset = dsets.FashionMNIST(\"ruta_donde_guardar_datos\", download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
    "\n",
    "# Cuantas imagenes obtener en cada iteracion!\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Creamos los loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Convolucional\n",
    "\n",
    "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arcquitectura y comportamiento de los componentes del modelo. \n",
    "\n",
    "En particular vamos a usar:\n",
    "\n",
    "- capas convolucionales de 2D (https://pytorch.org/docs/stable/nn.html#conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes). \n",
    "\n",
    "- Capas de maxpooling (https://pytorch.org/docs/stable/nn.html#maxpool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
    "\n",
    "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self, number_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pooling_layer = nn.MaxPool2d(kernel_size=2, stride=2)   # Regiones de 2x2 con paso 2.\n",
    "        \n",
    "        # Nuestras imagenes son de 28x28 y vamos a aplicar 2 veces la capa de pooling\n",
    "        # por lo que el resultado es de tamaño 7x7 (28 / 2 / 2).\n",
    "        # El 16 es porque terminamos con 16 feature maps de 7x7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=7*7*8, out_features=128) \n",
    "        \n",
    "        self.output = nn.Linear(in_features=128, out_features=number_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, new_input):\n",
    "        result = self.conv1(new_input)\n",
    "        result = F.relu(self.pooling_layer(result))\n",
    "        \n",
    "        result = self.conv2(result)\n",
    "        result = F.relu(self.pooling_layer(result))     \n",
    "        \n",
    "        # \"Achatamos\" los feature maps\n",
    "        result = result.reshape((-1, self.fc1.in_features))\n",
    "        result = F.relu(self.fc1(result))\n",
    "        \n",
    "        return self.output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 500 - Loss: 0.72002\n",
      "Validation accuracy 83.64 %\n",
      "Epoch: 2, Batch: 500 - Loss: 0.37568\n",
      "Validation accuracy 87.17 %\n",
      "Epoch: 3, Batch: 500 - Loss: 0.31730\n",
      "Validation accuracy 86.96 %\n",
      "Epoch: 4, Batch: 500 - Loss: 0.29100\n",
      "Validation accuracy 87.83 %\n",
      "Epoch: 5, Batch: 500 - Loss: 0.27695\n",
      "Validation accuracy 88.06 %\n",
      "Epoch: 6, Batch: 500 - Loss: 0.25950\n",
      "Validation accuracy 89.06 %\n",
      "Epoch: 7, Batch: 500 - Loss: 0.23986\n",
      "Validation accuracy 89.53 %\n",
      "Epoch: 8, Batch: 500 - Loss: 0.23964\n",
      "Validation accuracy 89.89 %\n",
      "Epoch: 9, Batch: 500 - Loss: 0.22123\n",
      "Validation accuracy 90.12 %\n",
      "Epoch: 10, Batch: 500 - Loss: 0.21364\n",
      "Validation accuracy 90.10 %\n",
      "Test set accuracy 89.72 %\n"
     ]
    }
   ],
   "source": [
    "conv_model = ConvolutionalModel(number_classes=10).to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "conv_model = train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=10)\n",
    "test_model(conv_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2\n",
    "\n",
    "Cree y entrene un modelo de red Convolucional que funcione mejor que el visto en clase. Puede usar lo que considere necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
