{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81eDC7bWC9y"
   },
   "source": [
    "# Métodos de Diferencias Temporales (TD)\n",
    "\n",
    "En este notebook vamos a ver métodos de diferencias temporales en particular, vamos a ver un método de control on-policy (Sarsa) y un método off-policy (Q-Learning) para estimar la funcion de valor (y la política) ótima para un problema. \n",
    "\n",
    "\n",
    "El notebook se basa en el capítulo 6 del libro de Sutton y Barto.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Vamos a utilizar un problema clásico de Reinforcement Learning, y para ello vamos a usar otro ambiente de OpenAi gym: MountainCar: https://gym.openai.com/envs/MountainCar-v0/ o https://gymnasium.farama.org/environments/classic_control/mountain_car/.\n",
    "\n",
    "La descripcion del mismo es la siguiente:\n",
    "\n",
    "Un auto esta posicionado en un carril de una dimension entre dos montañas. El objetivo es llegar a la cima de la montaña derecha pero, el motor del auto no es lo suficientemente fuerte para hacerlo en una sola pasada (no puede simplemente acelerar y llegar). Entonces, la única forma de tener éxito es ir de atrás hacia delante repetidas veces para acumular suficiente energía para subir.\n",
    "\n",
    "![Image](https://i.ytimg.com/vi/slIJHOuTCmc/hqdefault.jpg)\n",
    "\n",
    "Este ambiente tiene representación gráfica visual en gym, para ello vamos a hacer uso de unas funciones auxliares para poder ver resultados en video en colab.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## A entregar:\n",
    "\n",
    "- Implementación de algoritmo Q-Learning\n",
    "- Implementacion de algoritmo Sarsa\n",
    "- Comparacion entre ambos para el ambiente dado, cuanto tarda cada uno en llegar al objetivo (en promedio). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZAq-yLHeHzc"
   },
   "source": [
    "### Funciones auxiliares para visualizar el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.1.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.3.1\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/nicolasgrezzi/opt/anaconda3/envs/tallerIA\n",
      "\n",
      "  added / updated specs:\n",
      "    - moviepy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    aom-3.5.0                  |       hf0c8a7f_0         3.2 MB  conda-forge\n",
      "    expat-2.5.0                |       hf0c8a7f_1         118 KB  conda-forge\n",
      "    ffmpeg-5.1.2               | gpl_h8b4fe81_106         9.3 MB  conda-forge\n",
      "    gnutls-3.7.8               |       h207c4f0_0         2.1 MB  conda-forge\n",
      "    icu-72.1                   |       h7336db1_0        11.1 MB  conda-forge\n",
      "    imageio-2.27.0             |     pyh24c5eb1_0         3.1 MB  conda-forge\n",
      "    imageio-ffmpeg-0.4.8       |     pyhd8ed1ab_0          20 KB  conda-forge\n",
      "    lame-3.100                 |    hb7f2c08_1003         530 KB  conda-forge\n",
      "    libexpat-2.5.0             |       hf0c8a7f_1          68 KB  conda-forge\n",
      "    libidn2-2.3.4              |       hb7f2c08_0         170 KB  conda-forge\n",
      "    libopus-1.3.1              |       hc929b4f_1         273 KB  conda-forge\n",
      "    libtasn1-4.19.0            |       hb7f2c08_0         116 KB  conda-forge\n",
      "    libunistring-0.9.10        |       h0d85af4_0         1.3 MB  conda-forge\n",
      "    libvpx-1.11.0              |       he49afe7_3         1.4 MB  conda-forge\n",
      "    libxml2-2.10.4             |       h554bb67_0         621 KB  conda-forge\n",
      "    moviepy-1.0.3              |     pyhd8ed1ab_1          78 KB  conda-forge\n",
      "    nettle-3.8.1               |       h96f3785_1         535 KB  conda-forge\n",
      "    openh264-2.3.1             |       hf0c8a7f_2         656 KB  conda-forge\n",
      "    p11-kit-0.24.1             |       h65f8906_0         815 KB  conda-forge\n",
      "    proglog-0.1.9              |             py_0           9 KB  conda-forge\n",
      "    svt-av1-1.4.1              |       hf0c8a7f_0         2.3 MB  conda-forge\n",
      "    x264-1!164.3095            |       h775f41a_2         915 KB  conda-forge\n",
      "    x265-3.5                   |       hbb4e6a2_3         3.3 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        42.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  aom                conda-forge/osx-64::aom-3.5.0-hf0c8a7f_0 \n",
      "  expat              conda-forge/osx-64::expat-2.5.0-hf0c8a7f_1 \n",
      "  ffmpeg             conda-forge/osx-64::ffmpeg-5.1.2-gpl_h8b4fe81_106 \n",
      "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
      "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
      "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
      "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n",
      "  fontconfig         conda-forge/osx-64::fontconfig-2.14.2-h5bb23bf_0 \n",
      "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
      "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
      "  gettext            conda-forge/osx-64::gettext-0.21.1-h8a4c099_0 \n",
      "  gmp                conda-forge/osx-64::gmp-6.2.1-h2e338ed_0 \n",
      "  gnutls             conda-forge/osx-64::gnutls-3.7.8-h207c4f0_0 \n",
      "  icu                conda-forge/osx-64::icu-72.1-h7336db1_0 \n",
      "  imageio            conda-forge/noarch::imageio-2.27.0-pyh24c5eb1_0 \n",
      "  imageio-ffmpeg     conda-forge/noarch::imageio-ffmpeg-0.4.8-pyhd8ed1ab_0 \n",
      "  lame               conda-forge/osx-64::lame-3.100-hb7f2c08_1003 \n",
      "  libexpat           conda-forge/osx-64::libexpat-2.5.0-hf0c8a7f_1 \n",
      "  libiconv           conda-forge/osx-64::libiconv-1.17-hac89ed1_0 \n",
      "  libidn2            conda-forge/osx-64::libidn2-2.3.4-hb7f2c08_0 \n",
      "  libopus            conda-forge/osx-64::libopus-1.3.1-hc929b4f_1 \n",
      "  libtasn1           conda-forge/osx-64::libtasn1-4.19.0-hb7f2c08_0 \n",
      "  libunistring       conda-forge/osx-64::libunistring-0.9.10-h0d85af4_0 \n",
      "  libvpx             conda-forge/osx-64::libvpx-1.11.0-he49afe7_3 \n",
      "  libxml2            conda-forge/osx-64::libxml2-2.10.4-h554bb67_0 \n",
      "  moviepy            conda-forge/noarch::moviepy-1.0.3-pyhd8ed1ab_1 \n",
      "  nettle             conda-forge/osx-64::nettle-3.8.1-h96f3785_1 \n",
      "  openh264           conda-forge/osx-64::openh264-2.3.1-hf0c8a7f_2 \n",
      "  p11-kit            conda-forge/osx-64::p11-kit-0.24.1-h65f8906_0 \n",
      "  proglog            conda-forge/noarch::proglog-0.1.9-py_0 \n",
      "  svt-av1            conda-forge/osx-64::svt-av1-1.4.1-hf0c8a7f_0 \n",
      "  x264               conda-forge/osx-64::x264-1!164.3095-h775f41a_2 \n",
      "  x265               conda-forge/osx-64::x265-3.5-hbb4e6a2_3 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n",
      "zsh:1: no matches found: gymnasium[classic-control]\n"
     ]
    }
   ],
   "source": [
    "# nico conda\n",
    "# desde la consola:\n",
    "# Para ejecutar en CECOFI\n",
    "!conda install moviepy\n",
    "!conda install gymnasium[classic-control]\n",
    "\n",
    "# Dependencias necesarias.\n",
    "!conda install --upgrade gymnasium > /dev/null 2>&1\n",
    "!conda install --upgrade pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fNgTyITdb0e"
   },
   "outputs": [],
   "source": [
    "# Dependencias necesarias.\n",
    "!pip install --upgrade gymnasium > /dev/null 2>&1\n",
    "!pip install --upgrade pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ejecutar en CECOFI\n",
    "#!pip install moviepy\n",
    "#!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4dmKgVnGVXk0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvirtualdisplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_video\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordVideo\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyvirtualdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Display\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m \u001b[39mimport\u001b[39;00m display \u001b[39mas\u001b[39;00m ipythondisplay\n\u001b[1;32m     15\u001b[0m display \u001b[39m=\u001b[39m Display(visible\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m1400\u001b[39m, \u001b[39m900\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyvirtualdisplay'"
     ]
    }
   ],
   "source": [
    "# Imports y funciones para ver el ambiente.\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import io\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import glob\n",
    "import base64\n",
    "#from gym.wrappers import Monitor\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "  \n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia5jzW5Fd-Kq"
   },
   "source": [
    "### Creación del ambiente y visualizacion de un agente aleatorio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "qSKTaNhKd-PY",
    "outputId": "0f9fd943-99d2-45b4-b229-491d937edf42"
   },
   "outputs": [],
   "source": [
    "# Necesitamos \"envolver\" el ambiente para hacer uso de las funciones definidas anteriormente.\n",
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "# El resto del código es igual a lo que venimos acostumbrados.\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()  # Queremos poder ver el ambiente. \n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "        \n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXuu2iiGh-N1"
   },
   "source": [
    "### Interacción con el ambiente\n",
    "\n",
    "El ambiente cuenta con 3 acciones: Acelerar a la izquierda, frenar y acelerar a la derecha. Las recompensas son -1 por cada accion tomada con excepcion de llegar a la cima de la montaña. Un episodio se termina al alcanzar la cima o al luego de 200 interacciones.\n",
    "\n",
    "Lo que podemos observar al interactuar con el es un valor real para su Velocidad (negativa al ir para la izquierda, positiva a la derecha) y un valor real para su Posicion en la línea.\n",
    "\n",
    "Ambos valores son continuos, esto nos representa un problema, ya que queremos modelar una función de valor y una política para todos los estados posibles.\n",
    "\n",
    "Para atacar este problema vamos a **DISCRETIZAR** el ambiente, esto es: convertir la posicion y velocidad del auto en valores discretos dentro de un rango definido.\n",
    "\n",
    "Para ello vamos a definir algunas constantes y una función que nos permite obtener una representacion discreta de las observaciones del ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38xxdT-emM4u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will use 40 different values for Position and 40 for velocity (40x40 combinations)\n",
    "NUMBER_STATES = 40\n",
    "\n",
    "def discretization(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    env_den = (env_high - env_low) / NUMBER_STATES\n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "    \n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kMCuawcphJr"
   },
   "source": [
    "Una vez tenemos el ambiente discretizado, podemos crear una tabla que contenga el valor esperado para cada estado posible (donde en este caso, cada estado es una terna de: posicion, velocidad y accion a tomar). \n",
    "\n",
    "Esta tabla es normalmente conocida como \"Q table\" por su uso en Q learning (aunque tambien se usa, pero de manera distinta en Sarsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jL1Gl_ovepE"
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Vamos a comenzar implementando Q-learning para el problema actual, la implementacion corre por parte de los estudiantes.\n",
    "\n",
    "Recordamos aquí el algoritmo:\n",
    "\n",
    "![Image](https://leimao.github.io/images/blog/2019-03-14-RL-On-Policy-VS-Off-Policy/q-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiw-__ryvuoJ"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "  # Initialize Q_table as a matrix of NumberStatesxNumberStatesxNumberActions\n",
    "  \n",
    "  # Initialize epsilon\n",
    "  epsilon = epsilon_start\n",
    "  \n",
    "  # Stat trackers\n",
    "  # Track the accumulated reward per episode\n",
    "  # Track the number of steps taken in each episode\n",
    "  wins = []              # Track the number of times we reach the goal (1 if we do, 0 otherwise)\n",
    "\n",
    "  for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "\n",
    "    # Reset the environment and get initial position and velocity\n",
    "\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not (done or truncated):\n",
    "\n",
    "      # Epsilon greedy policy.\n",
    "      action = np.argmax(q_table[position][velocity])\n",
    "      if np.random.uniform() < epsilon:\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "      \n",
    "      # Take action on env and observe result.\n",
    "\n",
    "      # Q-Learning Update rule.\n",
    "      \n",
    "      # Move the state forward.\n",
    "    \n",
    "    # Count number of \"wins\" for mountain car env.\n",
    "    if done:\n",
    "      wins.append(1)\n",
    "    else:\n",
    "      wins.append(0)\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "  return q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "U4iL6nS5t2I5",
    "outputId": "13fdcf75-1a17-4954-81dc-70b3081f361d"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = q_learning(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "45FgL2C3TRql",
    "outputId": "3562b607-2a48-4752-8368-48eafef79fa2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCCsW_iJIfH"
   },
   "source": [
    "## Visualizacion del agente\n",
    "\n",
    "Vamos a ejecutar una política completamente greedy para observar lo aprendido por el algoritmo, usando la q_table retornada por el mismo para decidir la mejor accion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "rXDjSNfPt2M5",
    "outputId": "a35db66f-f3f3-42ee-d145-90ade034d9f9"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfYj49u7eseK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18GiSLEeKKtc"
   },
   "source": [
    "### Implementación de Sarsa\n",
    "\n",
    "\n",
    "![Image](https://miro.medium.com/max/2612/1*Wim9wr-jYJtZyZ_4ZJRHNg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeopCSvcUtt_"
   },
   "outputs": [],
   "source": [
    "def sarsa(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "  ## Your code here\n",
    "\n",
    "  return q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0raz4aUUuI7"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = sarsa(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "xhTXraYchNoY",
    "outputId": "d253b713-67c8-49c4-f58c-6f10fd517184"
   },
   "outputs": [],
   "source": [
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "pH4zGMaIhNqe",
    "outputId": "a01e976e-ee64-49f0-aa1d-de84774721e0"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
